{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1tshZmHbKLMEyjmUr-JBQ2BOu-hCfh74T","authorship_tag":"ABX9TyM6+1GhUt/4mraB2uN4jJx2"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11322394,"sourceType":"datasetVersion","datasetId":7081728}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport random\nimport urllib.request\nfrom tqdm import tqdm","metadata":{"id":"jBh9Y3qs6rRD","executionInfo":{"status":"ok","timestamp":1744035711821,"user_tz":-180,"elapsed":867,"user":{"displayName":"Muhammad Sanwal","userId":"05394980154394025644"}},"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T10:03:19.276201Z","iopub.execute_input":"2025-04-08T10:03:19.276530Z","iopub.status.idle":"2025-04-08T10:03:19.539144Z","shell.execute_reply.started":"2025-04-08T10:03:19.276505Z","shell.execute_reply":"2025-04-08T10:03:19.538505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------------------------\n# CONFIGURATION\n# ---------------------------------------------\n# Adjust these paths as per your setup\nTRAIN_CSV = \"/kaggle/input/dataset-google-landmark/train.csv\"\nTRAIN_CLEAN_CSV = \"/kaggle/input/dataset-google-landmark/train_clean.csv\"\nOUTPUT_DIR = \"/kaggle/tmp/landmark_images\"\nNUM_CLASSES = 50  # top N landmark classes\nIMAGES_PER_CLASS = 200  # images per class\n","metadata":{"id":"8F0qu_1N7XmW","executionInfo":{"status":"ok","timestamp":1744035791084,"user_tz":-180,"elapsed":7,"user":{"displayName":"Muhammad Sanwal","userId":"05394980154394025644"}},"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T10:07:15.007735Z","iopub.execute_input":"2025-04-08T10:07:15.008007Z","iopub.status.idle":"2025-04-08T10:07:15.012061Z","shell.execute_reply.started":"2025-04-08T10:07:15.007987Z","shell.execute_reply":"2025-04-08T10:07:15.011141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------------------------\n# SETUP\n# ---------------------------------------------\nos.makedirs(OUTPUT_DIR, exist_ok=True)","metadata":{"id":"3ZG4rBUx7Yb-","executionInfo":{"status":"ok","timestamp":1744035795260,"user_tz":-180,"elapsed":185,"user":{"displayName":"Muhammad Sanwal","userId":"05394980154394025644"}},"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T10:07:17.611233Z","iopub.execute_input":"2025-04-08T10:07:17.611561Z","iopub.status.idle":"2025-04-08T10:07:17.615653Z","shell.execute_reply.started":"2025-04-08T10:07:17.611530Z","shell.execute_reply":"2025-04-08T10:07:17.614776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------------------------\n# LOAD METADATA\n# ---------------------------------------------\nprint(\"[INFO] Loading metadata...\")\ntrain_df = pd.read_csv(TRAIN_CSV)\nclean_df = pd.read_csv(TRAIN_CLEAN_CSV)\n\n# Parse image IDs from string to list\nclean_df[\"images\"] = clean_df[\"images\"].apply(lambda x: x.split())\n\n# Sort by number of images and select top N classes\nclean_df[\"count\"] = clean_df[\"images\"].apply(len)\ntop_classes = clean_df.sort_values(by=\"count\", ascending=False).head(NUM_CLASSES)\n\n# Create a flat list of (image_id, landmark_id) pairs\nprint(f\"[INFO] Selecting {IMAGES_PER_CLASS} images from {NUM_CLASSES} classes...\")\nselected_images = []\nfor _, row in top_classes.iterrows():\n    landmark_id = row[\"landmark_id\"]\n    image_ids = row[\"images\"]\n    sampled = random.sample(image_ids, min(IMAGES_PER_CLASS, len(image_ids)))\n    for img_id in sampled:\n        selected_images.append((img_id, landmark_id))\n\nselected_df = pd.DataFrame(selected_images, columns=[\"id\", \"landmark_id\"])\n\n# Merge to get URLs\nmerged_df = pd.merge(selected_df, train_df, on=[\"id\", \"landmark_id\"], how=\"left\")","metadata":{"id":"iWVZwgbX7etA","executionInfo":{"status":"ok","timestamp":1744035819015,"user_tz":-180,"elapsed":19409,"user":{"displayName":"Muhammad Sanwal","userId":"05394980154394025644"}},"outputId":"d72375d1-ef79-4f74-8e79-7ce065c6866b","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T10:07:20.250397Z","iopub.execute_input":"2025-04-08T10:07:20.250759Z","iopub.status.idle":"2025-04-08T10:07:33.842807Z","shell.execute_reply.started":"2025-04-08T10:07:20.250732Z","shell.execute_reply":"2025-04-08T10:07:33.842114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------------------------\n# DOWNLOAD FUNCTION\n# ---------------------------------------------\nimport time\nimport urllib.error\nfrom urllib.request import Request, urlopen\n\ndef download_image(row, out_dir, retries=3, delay=2):\n    img_id = row[\"id\"]\n    url = row[\"url\"]\n    ext = url.split(\".\")[-1].lower()\n    if ext not in [\"jpg\", \"jpeg\", \"png\"]:\n        ext = \"jpg\"\n    save_path = os.path.join(out_dir, f\"{img_id}.{ext}\")\n\n    if os.path.exists(save_path):\n        return  # skip if already downloaded\n\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (compatible; landmark-dataset-downloader/1.0; +https://example.com)'\n    }\n\n    for attempt in range(retries):\n        try:\n            req = Request(url, headers=headers)\n            with urlopen(req, timeout=10) as response, open(save_path, 'wb') as out_file:\n                out_file.write(response.read())\n            return\n        except urllib.error.HTTPError as e:\n            if e.code == 429:\n                print(f\"[RETRY] {img_id}: Too many requests. Waiting {delay}s...\")\n                time.sleep(delay)\n                delay *= 2  # exponential backoff\n            else:\n                print(f\"[ERROR] {img_id}: HTTP error {e.code}\")\n                return\n        except Exception as e:\n            print(f\"[ERROR] {img_id}: {e}\")\n            return\n\n","metadata":{"id":"oDa4G59r7h0O","executionInfo":{"status":"ok","timestamp":1744031295752,"user_tz":-180,"elapsed":5,"user":{"displayName":"Muhammad Sanwal","userId":"05394980154394025644"}},"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T10:07:38.696927Z","iopub.execute_input":"2025-04-08T10:07:38.697204Z","iopub.status.idle":"2025-04-08T10:07:38.703951Z","shell.execute_reply.started":"2025-04-08T10:07:38.697184Z","shell.execute_reply":"2025-04-08T10:07:38.703097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------------------------\n# DOWNLOAD IMAGES\n# ---------------------------------------------\nprint(f\"[INFO] Downloading {len(merged_df)} images to: {OUTPUT_DIR}\")\nfor _, row in tqdm(merged_df.iterrows(), total=len(merged_df)):\n    download_image(row, OUTPUT_DIR)\n\n# ---------------------------------------------\n# SAVE IMAGE METADATA\n# ---------------------------------------------\nprint(\"[INFO] Saving metadata...\")\nmerged_df.to_csv(os.path.join(OUTPUT_DIR, \"downloaded_metadata.csv\"), index=False)\n\nprint(\"[DONE] Dataset curation complete.\")","metadata":{"id":"k8BTHJhMXhSA","executionInfo":{"status":"ok","timestamp":1744034819880,"user_tz":-180,"elapsed":3489744,"user":{"displayName":"Muhammad Sanwal","userId":"05394980154394025644"}},"outputId":"f28007c8-c4ca-41f2-ba5c-c84e169afe11","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T10:07:39.673182Z","iopub.execute_input":"2025-04-08T10:07:39.673480Z","iopub.status.idle":"2025-04-08T10:56:50.922577Z","shell.execute_reply.started":"2025-04-08T10:07:39.673458Z","shell.execute_reply":"2025-04-08T10:56:50.921814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# ---------------------------------------------\n# CONFIGURATION\n# ---------------------------------------------\nINPUT_DIR = \"/kaggle/tmp/landmark_images\"\nOUTPUT_JSON = \"/kaggle/working/landmarks_coco.json\"\nIMAGE_EXT = [\".jpg\", \".jpeg\", \".png\"]\n\n# ---------------------------------------------\n# HELPER FUNCTIONS\n# ---------------------------------------------\ndef load_images(input_dir):\n    return [os.path.join(input_dir, f) for f in os.listdir(input_dir) if os.path.splitext(f)[1].lower() in IMAGE_EXT]\n\ndef generate_annotations(model, image_paths):\n    annotations = []\n    images = []\n    ann_id = 1\n    transform = transforms.Compose([\n        transforms.ToTensor()\n    ])\n\n    for img_id, path in enumerate(tqdm(image_paths)):\n        image = Image.open(path).convert(\"RGB\")\n        tensor = transform(image).unsqueeze(0).cuda()\n        result = model(tensor)[0]\n\n        filename = os.path.basename(path)\n        width, height = image.size\n        images.append({\n            \"id\": img_id,\n            \"file_name\": filename,\n            \"width\": width,\n            \"height\": height\n        })\n\n        for box, score in zip(result['boxes'], result['scores']):\n            if score.item() < 0.5:\n                continue\n            x_min, y_min, x_max, y_max = box.tolist()\n            annotations.append({\n                \"id\": ann_id,\n                \"image_id\": img_id,\n                \"category_id\": 1,\n                \"bbox\": [x_min, y_min, x_max - x_min, y_max - y_min],\n                \"area\": (x_max - x_min) * (y_max - y_min),\n                \"iscrowd\": 0\n            })\n            ann_id += 1\n\n    return images, annotations\n\n# ---------------------------------------------\n# LOAD MODEL\n# ---------------------------------------------\nmodel = fasterrcnn_resnet50_fpn(pretrained=True).cuda()\nmodel.eval()\n\n# ---------------------------------------------\n# RUN DETECTION & SAVE COCO JSON\n# ---------------------------------------------\nimage_paths = load_images(INPUT_DIR)\nimages, annotations = generate_annotations(model, image_paths)\n\ncoco_json = {\n    \"info\": {\"description\": \"Landmark Dataset with Bounding Boxes\"},\n    \"licenses\": [],\n    \"images\": images,\n    \"annotations\": annotations,\n    \"categories\": [{\"id\": 1, \"name\": \"landmark\"}]\n}\n\nwith open(OUTPUT_JSON, \"w\") as f:\n    json.dump(coco_json, f)\n\nprint(\"[DONE] COCO JSON saved.\")","metadata":{"id":"kbMYzif8mDk-","executionInfo":{"status":"error","timestamp":1744043088005,"user_tz":-180,"elapsed":6803679,"user":{"displayName":"Muhammad Sanwal","userId":"05394980154394025644"}},"outputId":"86bf8616-1730-49bd-a720-288f04ba631f","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T12:11:28.712571Z","iopub.execute_input":"2025-04-08T12:11:28.712902Z","iopub.status.idle":"2025-04-08T13:10:12.207266Z","shell.execute_reply.started":"2025-04-08T12:11:28.712880Z","shell.execute_reply":"2025-04-08T13:10:12.206514Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step-by-Step Training Pipeline","metadata":{"id":"wbf4L8TwChqf"}},{"cell_type":"code","source":"import torch\n\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")\n","metadata":{"id":"7UME-vOLC2dn","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:24:49.254426Z","iopub.execute_input":"2025-04-08T13:24:49.254753Z","iopub.status.idle":"2025-04-08T13:24:49.262763Z","shell.execute_reply.started":"2025-04-08T13:24:49.254727Z","shell.execute_reply":"2025-04-08T13:24:49.261979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:30:02.042885Z","iopub.execute_input":"2025-04-08T13:30:02.043155Z","iopub.status.idle":"2025-04-08T13:30:02.047856Z","shell.execute_reply.started":"2025-04-08T13:30:02.043135Z","shell.execute_reply":"2025-04-08T13:30:02.047175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = models.resnet50(pretrained=True)\nmodel.fc = torch.nn.Linear(model.fc.in_features, num_classes)\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:30:13.748162Z","iopub.execute_input":"2025-04-08T13:30:13.748612Z","iopub.status.idle":"2025-04-08T13:30:14.421814Z","shell.execute_reply.started":"2025-04-08T13:30:13.748578Z","shell.execute_reply":"2025-04-08T13:30:14.420894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_baseline(model, dataloader, criterion, optimizer, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss, correct, total = 0.0, 0, 0\n        for inputs, labels in dataloader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        acc = correct / total\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss:.4f}, Accuracy: {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:30:28.121966Z","iopub.execute_input":"2025-04-08T13:30:28.122239Z","iopub.status.idle":"2025-04-08T13:30:28.127532Z","shell.execute_reply.started":"2025-04-08T13:30:28.122217Z","shell.execute_reply":"2025-04-08T13:30:28.126736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs, labels = next(iter(dataloader))\nprint(\"Inputs on:\", inputs.device)\nprint(\"Labels on:\", labels.device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T13:30:45.778752Z","iopub.execute_input":"2025-04-08T13:30:45.779071Z","iopub.status.idle":"2025-04-08T13:30:48.417659Z","shell.execute_reply.started":"2025-04-08T13:30:45.779045Z","shell.execute_reply":"2025-04-08T13:30:48.416757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport json\nfrom PIL import Image\n\n# ---------------------------------------------\n# DEVICE SETUP\n# ---------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---------------------------------------------\n# COCO Dataset Class\n# ---------------------------------------------\nclass LandmarkDataset(Dataset):\n    def __init__(self, coco_json_path, image_dir, transform=None):\n        with open(coco_json_path, 'r') as f:\n            self.coco = json.load(f)\n        self.image_dir = image_dir\n        self.transform = transform\n        self.annotations = self.coco['annotations']\n        self.images = {img['id']: img for img in self.coco['images']}\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        ann = self.annotations[idx]\n        img_info = self.images[ann['image_id']]\n        img_path = os.path.join(self.image_dir, img_info['file_name'])\n        img = Image.open(img_path).convert(\"RGB\")\n\n        # Crop to bounding box\n        x, y, w, h = ann['bbox']\n        img = img.crop((x, y, x + w, y + h))\n\n        if self.transform:\n            img = self.transform(img)\n\n        label = ann['category_id']\n        return img, label\n\n# ---------------------------------------------\n# Transformations\n# ---------------------------------------------\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# ---------------------------------------------\n# Training Function\n# ---------------------------------------------\ndef train_baseline(model, dataloader, criterion, optimizer, epochs=3):\n    model.train()\n    for epoch in range(epochs):\n        running_loss, correct, total = 0.0, 0, 0\n        for i, (inputs, labels) in enumerate(dataloader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            if i == 0:  # Print device info for first batch\n                print(f\"Inputs on: {inputs.device}, Labels on: {labels.device}, Model on: {next(model.parameters()).device}\")\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n        acc = correct / total\n        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {running_loss:.4f}, Accuracy: {acc:.4f}\")\n\n# ---------------------------------------------\n# Parameters\n# ---------------------------------------------\nnum_classes = 50  # Adjust based on your dataset\n\n# ---------------------------------------------\n# Load Dataset & DataLoader\n# ---------------------------------------------\ncoco_json_path = \"/kaggle/working/landmarks_coco.json\"\nimage_dir = \"/kaggle/tmp/landmark_images\"\n\ndataset = LandmarkDataset(coco_json_path, image_dir, transform)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n\n# ---------------------------------------------\n# Load and Prepare ResNet-50 Model\n# ---------------------------------------------\nmodel = models.resnet50(pretrained=True)\nmodel.fc = torch.nn.Linear(model.fc.in_features, num_classes)\nmodel = model.to(device)\n\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# ---------------------------------------------\n# Train the Model\n# ---------------------------------------------\ntrain_baseline(model, dataloader, criterion, optimizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T14:19:39.944494Z","iopub.execute_input":"2025-04-08T14:19:39.944846Z","iopub.status.idle":"2025-04-08T16:26:38.184483Z","shell.execute_reply.started":"2025-04-08T14:19:39.944812Z","shell.execute_reply":"2025-04-08T16:26:38.183296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model, \"/kaggle/working/resnet50_landmarks.pth\")\n#This saves the entire model object (including architecture and weights). To load it later:\n# model = torch.load(\"/kaggle/tmp/resnet50_landmarks.pth\")\n# model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:29:03.253282Z","iopub.execute_input":"2025-04-08T16:29:03.253692Z","iopub.status.idle":"2025-04-08T16:29:03.459269Z","shell.execute_reply.started":"2025-04-08T16:29:03.253658Z","shell.execute_reply":"2025-04-08T16:29:03.458469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Option 2: Save only the model weights (recommended for portability)\ntorch.save(model.state_dict(), \"/kaggle/working/resnet50_landmarks_state_dict.pth\")\n#To load it later:\n\n# model = models.resnet50(pretrained=False)\n# model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n# model.load_state_dict(torch.load(\"/kaggle/tmp/resnet50_landmarks_state_dict.pth\"))\n# model.to(device)\n# model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:29:19.635588Z","iopub.execute_input":"2025-04-08T16:29:19.635943Z","iopub.status.idle":"2025-04-08T16:29:19.806496Z","shell.execute_reply.started":"2025-04-08T16:29:19.635913Z","shell.execute_reply":"2025-04-08T16:29:19.805456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model\ntorch.save(model.state_dict(), \"/kaggle/working/resnet50_landmarks_state_dict.pth\")\nprint(\"Model weights saved to /kaggle/working/resnet50_landmarks_state_dict.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:30:09.817530Z","iopub.execute_input":"2025-04-08T16:30:09.817900Z","iopub.status.idle":"2025-04-08T16:30:10.091025Z","shell.execute_reply.started":"2025-04-08T16:30:09.817870Z","shell.execute_reply":"2025-04-08T16:30:10.090031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.onnx\n\n# Dummy input to trace the model\ndummy_input = torch.randn(1, 3, 224, 224).to(device)\n\n# Path to save the ONNX model\nonnx_path = \"/kaggle/working/resnet50_landmarks.onnx\"\n\n# Export the model\ntorch.onnx.export(\n    model,                      # model being run\n    dummy_input,                # model input (or a tuple for multiple inputs)\n    onnx_path,                  # where to save the model\n    export_params=True,         # store the trained parameter weights\n    opset_version=11,           # the ONNX version to export the model to\n    do_constant_folding=True,   # whether to execute constant folding for optimization\n    input_names=['input'],      # the model’s input names\n    output_names=['output'],    # the model’s output names\n    dynamic_axes={              # allow variable batch size\n        'input': {0: 'batch_size'},\n        'output': {0: 'batch_size'}\n    }\n)\n\nprint(f\"Model successfully exported to ONNX format at: {onnx_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:31:27.257287Z","iopub.execute_input":"2025-04-08T16:31:27.257699Z","iopub.status.idle":"2025-04-08T16:31:29.371048Z","shell.execute_reply.started":"2025-04-08T16:31:27.257671Z","shell.execute_reply":"2025-04-08T16:31:29.370159Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Dataset Loader with Bounding Box Cropping and traditional learning","metadata":{"id":"7_KOY2dKDegu"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport torchvision.transforms as transforms\n\ndef crop_with_bbox(image, bbox):\n    x, y, w, h = bbox\n    return image.crop((x, y, x + w, y + h))\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\ndef load_dataset_from_coco(coco_json_path, images_dir):\n    from pycocotools.coco import COCO\n    coco = COCO(coco_json_path)\n    dataset = []\n\n    for img_id in coco.getImgIds():\n        img_info = coco.loadImgs(img_id)[0]\n        ann_ids = coco.getAnnIds(imgIds=img_id)\n        anns = coco.loadAnns(ann_ids)\n\n        image_path = os.path.join(images_dir, img_info['file_name'])\n        image = Image.open(image_path).convert(\"RGB\")\n        for ann in anns:\n            cropped = crop_with_bbox(image, ann[\"bbox\"])\n            label = ann[\"category_id\"]\n            dataset.append((transform(cropped), label))\n    return dataset\n","metadata":{"id":"AOOcvQlZDgN3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models\n\ndef get_cnn_model(model_name=\"resnet50\", num_classes=100):\n    if model_name == \"resnet50\":\n        model = models.resnet50(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n    elif model_name == \"vgg16\":\n        model = models.vgg16(pretrained=True)\n        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n    return model\n","metadata":{"id":"S6Q1jU4vDlTm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Few-Shot Learning (ProtoNet, SiameseNet)\n","metadata":{"id":"hkizRVUNDyzS"}},{"cell_type":"code","source":"class ProtoNet(nn.Module):\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder\n\n    def forward(self, support, query):\n        support_emb = self.encoder(support)\n        query_emb = self.encoder(query)\n        proto = support_emb.view(n_way, k_shot, -1).mean(dim=1)\n        dists = euclidean_dist(query_emb, proto)\n        return -dists\n","metadata":{"id":"D9MyE6-8D1KG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def euclidean_dist(x, y):\n    n, m = x.size(0), y.size(0)\n    return ((x.unsqueeze(1) - y.unsqueeze(0))**2).sum(2)\n","metadata":{"id":"TdD9nHbuD3KC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training and Evaluation","metadata":{"id":"A4mRCCDFD7Ci"}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\n\ndef evaluate_model(model, dataloader, device):\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            preds = outputs.argmax(dim=1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n    print(classification_report(y_true, y_pred))\n","metadata":{"id":"nPDId9hZD9QA","trusted":true},"outputs":[],"execution_count":null}]}